# LMDeploy 部署与量化

## 配环境

可以复用 `L2 G4000` 创建的 `lmdeploy` 环境，可以顺利运行。


## 部署

启动API服务器

![image](https://github.com/user-attachments/assets/d374e067-8b1c-434e-a9bc-363a3d1948b6)

![image](https://github.com/user-attachments/assets/6a005de4-4cd3-4550-a9ec-4d4a9b0a4acf)

![image](https://github.com/user-attachments/assets/005aa68c-557e-4591-aeb3-0e16419dd10b)

## 量化
W4A16量化与kv cache量化

W4A16量化：
1. "W4"代表将模型的权重(Weights)量化为4位整数(int4)
2. "A16"表示激活值(Activations)保持16位精度(如FP16或BF16)
3. 主要优势是显著减少模型大小：
   - 例如将原本BF16精度的7B参数模型从14GB压缩到约3.5GB
   - 因为从16位(2字节)压缩到4位(0.5字节)，理论上可以减少到原来的1/4大小

KV Cache量化：
1. KV cache是一种缓存键值对(Key-Value pairs)的技术，用于复用计算结果以提高推理速度
2. KV Cache量化是指将这些缓存的数据进行压缩：
   - 可以选择int4(4位)或int8(8位)精度
   - 使用"per-head per-token"的非对称量化方式
3. 优势：
   - 在相同显存空间下可以存储更多数据
   - 例如用int4量化后，相同4GB显存可以存储BF16精度4倍的数据量

两种量化方式可以结合使用：
- W4A16量化减少模型本身占用的显存
- KV Cache量化减少缓存占用的显存

## 附录

1. 当你运行这个命令时，背后发生了以下步骤：

```bash
lmdeploy serve api_server /share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat --model-name internlm2_5-7b-chat
```

主要步骤：
- 模型加载：加载指定路径的模型文件到内存
- TurboMind初始化：设置推理引擎参数
- API服务器启动：
  - 创建FastAPI应用实例
  - 注册路由端点（endpoints）
  - 启动HTTP服务器（默认端口23333）
- WebUI初始化（如果启用）

2. http://0.0.0.0:23333/ 页面内容解析：
- 这是一个OpenAPI（Swagger UI）文档页面，显示了所有可用的API端点
- 主要部分：
  ```
  /v1/completions  # 用于单轮对话
  /v1/chat/completions  # 用于多轮对话
  /v1/models  # 查询可用模型信息
  ```
- 每个端点都包含：
  - HTTP方法（GET/POST）
  - 请求参数说明
  - 响应格式说明
  - 可以直接在页面测试API

3. 使用curl测试API：

基础调用：
```bash
# 查看可用模型
curl http://localhost:23333/v1/models

# 简单的聊天完成请求
curl -X POST http://localhost:23333/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "internlm2_5-7b-chat",
    "messages": [
      {"role": "user", "content": "你好"}
    ]
  }'
```

更复杂的示例：
```bash
# 多轮对话
curl -X POST http://localhost:23333/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "internlm2_5-7b-chat",
    "messages": [
      {"role": "user", "content": "你好"},
      {"role": "assistant", "content": "你好！有什么我可以帮你的吗？"},
      {"role": "user", "content": "请介绍一下你自己"}
    ],
    "temperature": 0.7,
    "max_tokens": 1024,
    "stream": false
  }'

# 流式输出（stream模式）
curl -X POST http://localhost:23333/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "internlm2_5-7b-chat",
    "messages": [
      {"role": "user", "content": "写一首诗歌"}
    ],
    "stream": true
  }'
```



