# LMDeploy 部署与量化

## 配环境

可以复用 `L2 G4000` 创建的 `lmdeploy` 环境，可以顺利运行。


## 部署

启动API服务器

![image](https://github.com/user-attachments/assets/d374e067-8b1c-434e-a9bc-363a3d1948b6)

![image](https://github.com/user-attachments/assets/6a005de4-4cd3-4550-a9ec-4d4a9b0a4acf)

![image](https://github.com/user-attachments/assets/005aa68c-557e-4591-aeb3-0e16419dd10b)

## 附录


1. 当你运行这个命令时，背后发生了以下步骤：

```bash
lmdeploy serve api_server /share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat --model-name internlm2_5-7b-chat
```

主要步骤：
- 模型加载：加载指定路径的模型文件到内存
- TurboMind初始化：设置推理引擎参数
- API服务器启动：
  - 创建FastAPI应用实例
  - 注册路由端点（endpoints）
  - 启动HTTP服务器（默认端口23333）
- WebUI初始化（如果启用）

2. http://0.0.0.0:23333/ 页面内容解析：
- 这是一个OpenAPI（Swagger UI）文档页面，显示了所有可用的API端点
- 主要部分：
  ```
  /v1/completions  # 用于单轮对话
  /v1/chat/completions  # 用于多轮对话
  /v1/models  # 查询可用模型信息
  ```
- 每个端点都包含：
  - HTTP方法（GET/POST）
  - 请求参数说明
  - 响应格式说明
  - 可以直接在页面测试API

3. 使用curl测试API：

基础调用：
```bash
# 查看可用模型
curl http://localhost:23333/v1/models

# 简单的聊天完成请求
curl -X POST http://localhost:23333/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "internlm2_5-7b-chat",
    "messages": [
      {"role": "user", "content": "你好"}
    ]
  }'
```

更复杂的示例：
```bash
# 多轮对话
curl -X POST http://localhost:23333/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "internlm2_5-7b-chat",
    "messages": [
      {"role": "user", "content": "你好"},
      {"role": "assistant", "content": "你好！有什么我可以帮你的吗？"},
      {"role": "user", "content": "请介绍一下你自己"}
    ],
    "temperature": 0.7,
    "max_tokens": 1024,
    "stream": false
  }'

# 流式输出（stream模式）
curl -X POST http://localhost:23333/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "internlm2_5-7b-chat",
    "messages": [
      {"role": "user", "content": "写一首诗歌"}
    ],
    "stream": true
  }'
```

调试技巧：
```bash
# 使用-v查看详细请求和响应信息
curl -v -X POST http://localhost:23333/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "internlm2_5-7b-chat",
    "messages": [
      {"role": "user", "content": "你好"}
    ]
  }'

# 保存响应到文件
curl -X POST http://localhost:23333/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d @request.json \
  -o response.json

# 设置超时时间
curl --max-time 30 -X POST http://localhost:23333/v1/chat/completions ...
```

常用参数说明：
- `temperature`：随机性程度（0-1）
- `max_tokens`：最大生成令牌数
- `stream`：是否启用流式输出
- `top_p`：核采样阈值
- `frequency_penalty`：频率惩罚系数
- `presence_penalty`：存在惩罚系数

建议：
1. 先使用Swagger UI页面测试API，确认参数正确
2. 然后再使用curl进行自动化测试
3. 对于复杂的请求体，建议保存在单独的JSON文件中
4. 使用`jq`工具处理JSON响应：
```bash
curl ... | jq .choices[0].message.content
```
