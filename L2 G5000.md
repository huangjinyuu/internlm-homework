![mermaid-diagram-2024-11-03-001201](https://github.com/user-attachments/assets/f2fd7bfd-21f0-4ae0-83f8-3eec3a5cdd1a)# 茴香豆

##  本地部署

### 配环境

```
# 复制环境
studio-conda -o internlm-base -t huixiangdou

# 激活环境
conda activate huixiangdou

# 安装软件包
apt update
apt install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig libpulse-dev

# python依赖
pip install BCEmbedding==0.15 cmake==3.30.2 lit==18.1.8 sentencepiece==0.2.0 protobuf==5.27.3 accelerate==0.33.0

git clone https://github.com/internlm/huixiangdou && cd huixiangdou
git checkout 79fa810
pip install -r requirements.txt
```

### 茴香豆的配置文件
`config.ini` 这个配置文件的主要包含了几个主要部分：

1. 特征存储（Feature Store）配置：
    - 使用嵌入模型 (embedding model) 来处理文本向量化，支持本地路径、HuggingFace 模型或 URL
    - 包含重排序模型 (reranker model) 配置
    - 设置了相关的 API 令牌和工作目录

2. 网络搜索配置（Web Search）：
    - 支持两种搜索引擎：DuckDuckGo Search (ddgs) 和 Serper
    - 可以设置域名的优先顺序
    - 配置结果保存目录

3. LLM（大语言模型）配置：
    - 支持本地和远程 LLM 服务
    - 本地 LLM 支持多种模型，如 InternLM2、Qwen 等
    - 远程 LLM 支持多个服务商：如GPT等

4. 工作节点（Worker）配置：
    - 可以启用/禁用网络搜索
    - 可以启用/禁用搜索增强
    - 支持时间控制（可以设置工作时间段）

5. Sourcegraph 搜索配置：
    - Sourcegraph 搜索配置是这个系统中一个特殊的功能模块，用于在特定代码仓库中进行智能搜索和检索。
    - 当用户询问相关项目的问题时，系统可以直接搜索这些预配置仓库中的代码和文档

7. 前端（Frontend）配置：
    - 支持多种群聊助手类型：
      - 飞书群（Lark Group）
      - 个人微信
      - 企业微信
    - 包含消息处理策略
    - 详细的微信群配置，包括多个"茴香豆"用户体验群

这个配置文件展示了一个功能完整的 RAG 系统，集成了：
- 文本向量化和检索
- 本地/远程 LLM 服务
- 网络搜索能力
- 多平台的对话接口
- 代码库搜索功能

---

从这里我们看出茴香豆和 `L1G4000` 课程 `LlamaIndex RAG` 的明显区别， `LlamaIndex RAG` 是一个比较naive的 RAG 实现，缺少了一些高级特性：
- 没有重排序（Reranker）步骤
- 仅使用向量相似度进行检索
- 没有网络搜索能力
- 没有多数据源整合

---

**了解重排**：

对初始检索（通常使用向量近邻搜索）的粗排，**重排序**会进行更细致的相关性评估

工作流程：
```
查询 → 初步检索(embedding检索) → 候选文档池 → 重排序模型 → 最终排序结果
```

为什么需要重排序：
- 初始检索（embedding 检索）使用向量相似度，这是一种较为粗略的匹配：
  - 只能捕捉到文本的整体语义相似度
  - 可能会忽略细节差异
  - 计算速度快，但准确度有限

- 重排序模型则可以：
  - 进行更细致的语义理解
  - 考虑更多上下文信息
  - 评估查询和文档之间的实际相关性
  - 处理细微的语义差异

具体应用场景举例：

假设用户问："PyTorch中如何实现批量归一化？"

初步检索可能返回：
```
1. 批量归一化的数学原理介绍
2. PyTorch中的BatchNorm使用示例
3. 深度学习中的归一化方法比较
4. 某个使用了BatchNorm的模型代码
```

重排序模型会：
- 详细分析每个文档与问题的相关性
- 考虑实际使用场景的匹配度
- 可能将"PyTorch中的BatchNorm使用示例"排到最前面，降低纯理论介绍的排序

配置文件中的相关设置：
```toml
# text2vec model, support local relative path, huggingface repo and URL.
# for example:
#  "maidalun1020/bce-embedding-base_v1"
#  "BAAI/bge-m3"
#  "https://api.siliconflow.cn/v1/embeddings"
embedding_model_path = "maidalun1020/bce-embedding-base_v1"

# reranker model, support list:
#  "maidalun1020/bce-reranker-base_v1"
#  "BAAI/bge-reranker-v2-minicpm-layerwise"
#  "https://api.siliconflow.cn/v1/rerank"
reranker_model_path = "maidalun1020/bce-reranker-base_v1"
```

Reranker 与 Embedding 模型的区别：
- Embedding 模型的输入是文本，输出是文本嵌入向量。
- Reranker 模型是输入是一对文本，输出是相似度衡量。

```
文本 -> Embedding模型 -> 向量表示(如768维向量)
(查询文本, 候选文本) -> Reranker模型 -> 相关性分数(0-1)
```


### 建立知识库

建立知识库的核心代码是 `/root/huixiangdou/huixiangdou/service/feature_store.py` 

![mermaid-diagram-2024-11-03-001201](https://github.com/user-attachments/assets/511ed3f3-5d2e-43f3-bf85-848f014e45ae)

这个 `feature_store.py` 是一个典型的 RAG 系统中负责文档处理和向量存储的模块，让我详细分析一下它的主要功能：

1. 关键处理流程：

```
# 1. 文档预处理
- 支持多种文档格式（PDF、Word、Excel、Markdown等）
- 将文档统一转换为文本格式
- 使用多进程处理文件（Pool(processes=8)）

# 2. 文本分块
- 中文使用 ChineseRecursiveTextSplitter
- 英文使用 RecursiveCharacterTextSplitter
- 支持设置块大小和重叠长度
chunk_size=900, chunk_overlap=32

# 3. 特征提取
- 使用embedder模型将文本块转换为向量
- 支持文本和图像模态
- 构建Faiss索引进行存储

# 4. 检索功能
- 实现了拒绝机制（reject pipeline）
- 支持相似度检索
- 包含缓存机制（CacheRetriever）
```

2. 重要组件：

```python
# Embedder处理
self.embedder = embedder  # 向量化模型

# 文本分割器
if language == 'zh':
    self.text_splitter = ChineseRecursiveTextSplitter(
        keep_separator=True,
        is_separator_regex=True,
        chunk_size=chunk_size,
        chunk_overlap=32)
else:
    self.text_splitter = RecursiveCharacterTextSplitter(...)

# 向量存储
Faiss.save_local(folder_path=feature_dir, 
                 chunks=filtered_chunks, 
                 embedder=self.embedder)
```

3. 特色功能：

```python
# 拒绝机制
- 通过阈值控制是否接受查询
- 使用good_questions和bad_questions训练阈值
retriever.update_throttle(
    config_path=args.config_path,
    good_questions=good_questions,
    bad_questions=bad_questions
)

# 文档分析
def analyze(self, chunks):
    # 分析文档长度分布
    # 分析token长度分布
    text_lens = []
    token_lens = []
    ...
    logger.info('text histogram, {}'.format(histogram(text_lens)))
```

4. 测试功能：

```python
def test_reject(retriever, sample):
    # 测试拒绝机制
    for example in real_questions:
        relative, score = retriever.is_relative(example)

def test_query(retriever, sample):
    # 测试检索功能
    chunks, context, refs = retriever.query(example)
```

5. 主要特点：

```python
a. 模块化设计
- 文档处理
- 向量化
- 索引构建
- 检索查询
都是独立模块

b. 多格式支持
files._type in ['pdf', 'word', 'excel', 'ppt', 'html', 'md', 'text']

c. 性能优化
- 多进程文件处理
- 向量索引
- 检索缓存

d. 质量控制
- 拒绝机制
- 文档分析
- 测试功能
```

这个模块在 RAG 系统中扮演了非常重要的角色：
1. 处理和规范化各种格式的输入文档
2. 将文档切分成适当大小的块
3. 使用向量模型提取特征
4. 构建高效的检索索引
5. 提供智能的拒绝机制

它是一个完整的文档处理和向量存储管理系统，为上层的问答功能提供了基础设施。


![image](https://github.com/user-attachments/assets/a3cfc5c7-9516-44fa-9bc8-bb2fd092381f)


### 测试效果

