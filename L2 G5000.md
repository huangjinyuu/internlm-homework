# 茴香豆

##  本地部署

### 配环境

```
# 复制环境
studio-conda -o internlm-base -t huixiangdou

# 激活环境
conda activate huixiangdou

# 安装软件包
apt update
apt install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig libpulse-dev

# python依赖
pip install BCEmbedding==0.15 cmake==3.30.2 lit==18.1.8 sentencepiece==0.2.0 protobuf==5.27.3 accelerate==0.33.0

git clone https://github.com/internlm/huixiangdou && cd huixiangdou
git checkout 79fa810
pip install -r requirements.txt
```

### 茴香豆的配置文件
`config.ini` 这个配置文件的主要包含了几个主要部分：

1. 特征存储（Feature Store）配置：
    - 使用嵌入模型 (embedding model) 来处理文本向量化，支持本地路径、HuggingFace 模型或 URL
    - 包含重排序模型 (reranker model) 配置
    - 设置了相关的 API 令牌和工作目录

2. 网络搜索配置（Web Search）：
    - 支持两种搜索引擎：DuckDuckGo Search (ddgs) 和 Serper
    - 可以设置域名的优先顺序
    - 配置结果保存目录

3. LLM（大语言模型）配置：
    - 支持本地和远程 LLM 服务
    - 本地 LLM 支持多种模型，如 InternLM2、Qwen 等
    - 远程 LLM 支持多个服务商：如GPT等

4. 工作节点（Worker）配置：
    - 可以启用/禁用网络搜索
    - 可以启用/禁用搜索增强
    - 支持时间控制（可以设置工作时间段）

5. Sourcegraph 搜索配置：
    - Sourcegraph 搜索配置是这个系统中一个特殊的功能模块，用于在特定代码仓库中进行智能搜索和检索。
    - 当用户询问相关项目的问题时，系统可以直接搜索这些预配置仓库中的代码和文档

7. 前端（Frontend）配置：
    - 支持多种群聊助手类型：
      - 飞书群（Lark Group）
      - 个人微信
      - 企业微信
    - 包含消息处理策略
    - 详细的微信群配置，包括多个"茴香豆"用户体验群

这个配置文件展示了一个功能完整的 RAG 系统，集成了：
- 文本向量化和检索
- 本地/远程 LLM 服务
- 网络搜索能力
- 多平台的对话接口
- 代码库搜索功能

---

从这里我们看出茴香豆和 `L1G4000` 课程 `LlamaIndex RAG` 的明显区别， `LlamaIndex RAG` 是一个比较naive的 RAG 实现，缺少了一些高级特性：
- 没有重排序（Reranker）步骤
- 仅使用向量相似度进行检索
- 没有网络搜索能力
- 没有多数据源整合

---

**了解重排**：

对初始检索（通常使用向量近邻搜索）的粗排，**重排序**会进行更细致的相关性评估

工作流程：
```
查询 → 初步检索(embedding检索) → 候选文档池 → 重排序模型 → 最终排序结果
```

为什么需要重排序：
- 初始检索（embedding 检索）使用向量相似度，这是一种较为粗略的匹配：
  - 只能捕捉到文本的整体语义相似度
  - 可能会忽略细节差异
  - 计算速度快，但准确度有限

- 重排序模型则可以：
  - 进行更细致的语义理解
  - 考虑更多上下文信息
  - 评估查询和文档之间的实际相关性
  - 处理细微的语义差异

具体应用场景举例：

假设用户问："PyTorch中如何实现批量归一化？"

初步检索可能返回：
```
1. 批量归一化的数学原理介绍
2. PyTorch中的BatchNorm使用示例
3. 深度学习中的归一化方法比较
4. 某个使用了BatchNorm的模型代码
```

重排序模型会：
- 详细分析每个文档与问题的相关性
- 考虑实际使用场景的匹配度
- 可能将"PyTorch中的BatchNorm使用示例"排到最前面，降低纯理论介绍的排序

配置文件中的相关设置：
```toml
# text2vec model, support local relative path, huggingface repo and URL.
# for example:
#  "maidalun1020/bce-embedding-base_v1"
#  "BAAI/bge-m3"
#  "https://api.siliconflow.cn/v1/embeddings"
embedding_model_path = "maidalun1020/bce-embedding-base_v1"

# reranker model, support list:
#  "maidalun1020/bce-reranker-base_v1"
#  "BAAI/bge-reranker-v2-minicpm-layerwise"
#  "https://api.siliconflow.cn/v1/rerank"
reranker_model_path = "maidalun1020/bce-reranker-base_v1"
```

Reranker 与 Embedding 模型的区别：
- Embedding 模型的输入是文本，输出是文本嵌入向量。
- Reranker 模型是输入是一对文本，输出是相似度衡量。

```
文本 -> Embedding模型 -> 向量表示(如768维向量)
(查询文本, 候选文本) -> Reranker模型 -> 相关性分数(0-1)
```


### 建立知识库

建立知识库的核心代码是 `/root/huixiangdou/huixiangdou/service/feature_store.py` ，负责文档处理和向量存储，总体逻辑如下：

![mermaid-diagram-2024-11-03-001201](https://github.com/user-attachments/assets/511ed3f3-5d2e-43f3-bf85-848f014e45ae)

feature_store.py 背后的逻辑：

1. 文档处理流程：
```
扫描目录 (repodir/) 
→ 支持类型：pdf/word/excel/ppt/html/md/text
→ 其他类型（如 .py）会被跳过
→ 文本提取是通过调用特定的解析器完成的
→ 统一转换为文本格式存储到 workdir/preprocess/
```

2. 文本分块：
```
预处理文本 → 智能分块
- 优先在语义边界处分割（段落、句子、标点）
- 不会随意切断句子，不会破坏语义
- 默认大小：chunk_size=900, overlap=32
- 确保上下文连贯性
```

3. 向量化和索引：
```
文本块 → Embedding向量 → Faiss索引
- 每个文本块生成一个向量（如768维）
- Faiss索引的本质：向量->文本块的映射关系
- 作用：快速找到最相似的文本块
```

4. 拒绝机制：
```
用户查询 → 计算相似度 → 判断是否使用知识库
- 拒绝机制可以选择性地使用知识库
- 如果相似度 > 阈值：检索知识库增强回答
- 如果相似度 < 阈值：直接让LLM回答
- 阈值通过好/坏问题样本优化得到
```

![image](https://github.com/user-attachments/assets/8bcc4822-456e-4aa8-a1ed-8ca5eb46337d)

![image](https://github.com/user-attachments/assets/26278048-6241-4e98-a39b-27835c2904f0)


---

```
workdir/                    # 工作目录
    ├── preprocess/         # 预处理文件，原始文档的文本形式
    │   ├── paper.text        # PDF转换后的文本
    │   ├── docs_install.md    # 复制的markdown文件
    │   └── api_guide.text     # Word文档转换的文本
    ├── kg/                    # 知识图谱目录（空，没用到）
    └── db_dense/               # 向量数据库，向量化后的检索系统
        ├── embedding.faiss    # Faiss向量索引文件(二进制)
        └── chunks_and_strategy.pkl
```

总体数据流向：
```
原始文件 -> 预处理文本 -> 文本块 -> 向量 -> Faiss索引
```

---

我在 `repodir` 下载了一些其他课程的 repo 构建知识库：

![image](https://github.com/user-attachments/assets/95822e90-9512-46b8-b079-642d93d770a2)

建立知识库运行成功：

![image](https://github.com/user-attachments/assets/a3cfc5c7-9516-44fa-9bc8-bb2fd092381f)


### 测试效果
命令行运行:

![image](https://github.com/user-attachments/assets/e35e7b0c-e356-43c6-bd45-31d2fce3c2d0)

---

Gradio UI 界面测试

问题一：
```
mindsearch里面怎么配置搜索引擎？
```

![image](https://github.com/user-attachments/assets/9b2720e1-26c2-4c6a-b181-39fed5912647)

![image](https://github.com/user-attachments/assets/773bb813-9d31-4b49-b0a6-f0a19c2a18e8)

---

问题二：

```
xtuner怎么进行模型转换与模型合并？
```

![image](https://github.com/user-attachments/assets/756d692b-52e1-40ca-ac6e-d5e0320d2e52)
