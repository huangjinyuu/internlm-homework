# 基于 LlamaIndex 构建个人 RAG 知识库


## 1. 构建RAG过程
### 1.1 配环境：安装很多包

1. 创建conda环境：`conda create -n llamaindex python=3.10`
2. 激活这个环境：`conda activate llamaindex`
3. 在这个环境，**疯狂**地安装**超级多**依赖（这个过程会有点慢，下载慢安装也慢，我还以为是机子卡了）
    ```
    conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia
    pip install einops==0.7.0 protobuf==5.26.1
    pip install llama-index==0.10.38 llama-index-llms-huggingface==0.2.0 "transformers[torch]==4.41.1" "huggingface_hub[inference]==0.23.1" huggingface_hub==0.23.1 sentence-transformers==2.7.0 sentencepiece==0.2.0
    ```
4. 通过教程给的脚本，下载 Sentence Transformer 模型
5. 下载 NLTK 相关资源
    - 为什么要这样做？由于访问 NLTK 官方仓库可能存在不稳定性，预先从国内镜像（如 Gitee）下载并配置所需的 NLTK 资源，可以确保项目的顺利进行，避免因网络问题导致的资源下载失败。
    - NLTK是怎么找到这个路径的？当您使用 NLTK 的某个功能（如分词、词性标注等）时，NLTK 会按照一定的顺序在预设的多个路径中查找所需的资源文件。默认情况下，nltk.data.path 包含以下路径（优先级从高到低）：
        - 当前工作目录的 nltk_data 文件夹。
        - 用户主目录下的 nltk_data 文件夹（如 ~/nltk_data）。
        - 系统级的 NLTK 数据目录（可能因操作系统不同而异）。
     如果 NLTK 在这些路径中找不到所需的资源，它会尝试从在线仓库下载。
    - 我们的 nltk_data 文件夹就在 `~/nltk_data`
    - 我们将资源预先下载并解压到正确的目录，确保 NLTK 可以直接访问这些资源，避免触发在线下载


### 1.2 使用 llama_index 库中的 HuggingFaceLLM 类直接推理
作为对照组，展示RAG之前的模型原生的效果。

这个推理LLM的脚本代码，和我们常见的基于transformers库的LLM推理代码长得很不一样：
```
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.llms import ChatMessage
llm = HuggingFaceLLM(
    model_name="/root/model/internlm2-chat-1_8b",
    tokenizer_name="/root/model/internlm2-chat-1_8b",
    model_kwargs={"trust_remote_code":True},
    tokenizer_kwargs={"trust_remote_code":True}
)
rsp = llm.chat(messages=[ChatMessage(content="xtuner是什么？")])
print(rsp)
```
LlamaIndex库提供了比 transformers 库更高层次的抽象和接口，从而简化了与大语言模型（LLM）的交互过程。

### 1.3 通过LlamaIndex构建RAG

### 1.4 构建Web UI

## 2. 结果验证

目标：解答私域问题：
```
武大超算怎么建立端口映射？
```

该问题在使用 LlamaIndex 之前InternLM2-Chat-1.8B模型不会回答：

通过构建 `武大超算的使用手册 知识库` ，借助 LlamaIndex 后，InternLM2-Chat-1.8B 模型具备回答目标问题的能力：
