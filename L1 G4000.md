# 基于 LlamaIndex 构建个人 RAG 知识库


## 1. 构建RAG过程
### 1.1 配环境：安装很多包

1. 创建conda环境：`conda create -n llamaindex python=3.10`
2. 激活这个环境：`conda activate llamaindex`
3. 在这个环境，**疯狂**地安装**超级多**依赖（这个过程会有点慢，下载慢安装也慢，我还以为是机子卡了）
    ```
    conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia
    pip install einops==0.7.0 protobuf==5.26.1
    pip install llama-index==0.10.38 llama-index-llms-huggingface==0.2.0 "transformers[torch]==4.41.1" "huggingface_hub[inference]==0.23.1" huggingface_hub==0.23.1 sentence-transformers==2.7.0 sentencepiece==0.2.0
    ```
4. 通过教程给的脚本，下载 Sentence Transformer 模型
5. 下载 NLTK 相关资源
    - **为什么要这样做**？由于访问 NLTK 官方仓库可能存在不稳定性，预先从国内镜像（如 Gitee）下载并配置所需的 NLTK 资源，可以确保项目的顺利进行，避免因网络问题导致的资源下载失败。
    - **NLTK是怎么找到这个路径的**？当您使用 NLTK 的某个功能（如分词、词性标注等）时，NLTK 会按照一定的顺序在预设的多个路径中查找所需的资源文件。默认情况下，nltk.data.path 包含以下路径（优先级从高到低）：
        - 当前工作目录的 nltk_data 文件夹。
        - 用户主目录下的 nltk_data 文件夹（如 ~/nltk_data）。
        - 系统级的 NLTK 数据目录（可能因操作系统不同而异）。
     如果 NLTK 在这些路径中找不到所需的资源，它会尝试从在线仓库下载。
    - 我们的 nltk_data 文件夹就在 `~/nltk_data`
    - 我们将资源预先下载并解压到正确的目录，确保 NLTK 可以直接访问这些资源，避免触发在线下载



### 1.2 使用 llama_index 库中的 HuggingFaceLLM 类直接推理
作为对照组，展示RAG之前的模型原生的效果。

这个推理LLM的脚本代码，和我们常见的基于transformers库的LLM推理代码长得很不一样：
```
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.llms import ChatMessage
llm = HuggingFaceLLM(
    model_name="/root/model/internlm2-chat-1_8b",
    tokenizer_name="/root/model/internlm2-chat-1_8b",
    model_kwargs={"trust_remote_code":True},
    tokenizer_kwargs={"trust_remote_code":True}
)
rsp = llm.chat(messages=[ChatMessage(content="xtuner是什么？")])
print(rsp)
```
LlamaIndex库提供了比 transformers 库更高层次的抽象和接口，从而简化了与大语言模型（LLM）的交互过程。

运行结果：
![image](https://github.com/user-attachments/assets/c05927aa-4393-4667-93c7-af03e9c9a70f)


---
<details>
    <summary>推理时遇到了cuda报错</summary>
    
    ```
    RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling cublasCreate(handle)
    ```

    不知道我啥时候装错了torch的版本导致了,低版本cuda无法支持高版本torch：

    解决办法：重新安装torch
    
    ```
    conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia
    ```
    

### 1.3 通过LlamaIndex构建RAG

### 1.4 构建Web UI

## 2. 结果验证

目标：解答私域问题：
```
武大超算怎么建立端口映射？
```

该问题在使用 LlamaIndex 之前InternLM2-Chat-1.8B模型不会回答：

通过构建 `武大超算的使用手册 知识库` ，借助 LlamaIndex 后，InternLM2-Chat-1.8B 模型具备回答目标问题的能力：


## 附录
关于这个教程文档，我觉得写得不好，

文档中的配置环境的顺序是这样的：
```
# 1. 创建并激活环境
conda create -n llamaindex python=3.10
conda activate llamaindex

# 2. 首先安装特定版本的 PyTorch，并确保使用 CUDA 11.7
conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia

# 3. 安装基础依赖
pip install einops==0.7.0 protobuf==5.26.1

# 4. 安装 LLamaIndex 相关
pip install llama-index==0.10.38 
pip install llama-index-llms-huggingface==0.2.0
pip install "transformers[torch]==4.41.1"
pip install "huggingface_hub[inference]==0.23.1" huggingface_hub==0.23.1
pip install sentence-transformers==2.7.0 
pip install sentencepiece==0.2.0

# 5. 安装向量依赖包
pip install llama-index-embeddings-huggingface==0.2.0 
pip install llama-index-embeddings-instructor==0.1.3
```
**存在的问题是，其中LLamaIndex 相关会稳定触发torch的升级**
```
pip install llama-index==0.10.38 llama-index-llms-huggingface==0.2.0 "transformers[torch]==4.41.1" "huggingface_hub[inference]==0.23.1" huggingface_hub==0.23.1 sentence-transformers==2.7.0 sentencepiece==0.2.0

pip install llama-index-embeddings-huggingface==0.2.0 llama-index-embeddings-instructor==0.1.3
```


这两条命令，会自动把我原来的匹配cuda11.7的torch版本升级为>>> import torch >>> print(torch.version) 2.5.1+cu124 >>> print(torch.version.cuda) 12.4

升级之后torch版本与cuda将不匹配会报错。

也就是说安装教程的顺序走肯定会在用torch的时候遇到cuda报错。

有什么解决方案？
