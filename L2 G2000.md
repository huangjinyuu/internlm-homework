# Lagent

## 配环境

```
# 创建环境
conda create -n agent_camp3 python=3.10 -y
# 激活环境
conda activate agent_camp3
# 安装 torch
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia -y
# 安装其他依赖包
pip install termcolor==2.4.0
pip install lmdeploy==0.5.2

# 安装lagent
mkdir -p /root/agent_camp3
cd /root/agent_camp3
git clone https://github.com/InternLM/lagent.git
cd lagent && git checkout 81e7ace && pip install -e . && cd ..
pip install griffe==0.48.0
```

## 体验 Lagent Web Demo


### 简单体验
第一步，通过 `lmdeploy` 部署一个 `api_server` :
```
lmdeploy serve api_server /share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat --model-name internlm2_5-7b-chat
```

![image](https://github.com/user-attachments/assets/b4414694-7c5c-4739-99f5-834f1b17aef0)

第二步，启动 `Lagent` 的Web Demo：
```
cd /root/agent_camp3/lagent
streamlit run examples/internlm2_agent_web_demo.py
```

![image](https://github.com/user-attachments/assets/18695f9e-f259-4fb1-8bc4-6b4b3f8addfc)

第三步，映射这两个端口到本地：
```
ssh -p 47155 root@ssh.intern-ai.org.cn -CNg -L 8501:127.0.0.1:8501 -L 23333:127.0.0.1:23333 -o StrictHostKeyChecking=no
```
然后打开web UI，填上模型名称、端口，选上插件。然后就可以玩耍了：

![image](https://github.com/user-attachments/assets/fe7da34b-5d1f-4b27-b88f-ec3cf46efe09)

![image](https://github.com/user-attachments/assets/b0e7f6ed-88a7-40ff-a6bf-feeeead82b71)


### 简单分析逻辑
从 `Lagent Web Demo` 的代码中浅尝逻辑。

首先，从代码中可以看到插件是在初始化时就注册好的：

```python
# SessionState.init_state() 中
action_list = [
    ArxivSearch(),  # 预先注册了 ArxivSearch 插件
]
st.session_state['plugin_map'] = {
    action.name: action
    for action in action_list
}
```

Chatbot 的初始化也和插件有关：
```python
# StreamlitUI.initialize_chatbot() 中
return Internlm2Agent(
    llm=model,
    protocol=Internlm2Protocol(
        tool=dict(
            begin='{start_token}{name}\n',
            start_token='<|action_start|>',
            name_map=dict(
                plugin='<|plugin|>', 
                interpreter='<|interpreter|>'
            ),
            belong='assistant',
            end='<|action_end|>\n',
        ),
    ),
    max_turn=7
)
```

这里的关键是定义了一个特殊的格式来标记工具调用，使用了特殊的 token：
- `<|action_start|>` - 动作开始
- `<|plugin|>` - 插件调用
- `<|action_end|>` - 动作结束

让我用一个序列图来展示完整的交互过程：

![image](https://github.com/user-attachments/assets/3f7386fc-d315-412b-bcaf-a9ab827c0748)


让我梳理一下完整的 LLM-插件交互周期：

1. LLM 决定使用插件：
- LLM 根据用户输入和系统提示词，判断是否需要使用插件
- 如果需要，生成包含特殊token和JSON格式参数的响应

2. Agent 解析和执行：
- 检测到特殊token时，解析JSON参数
- 查找对应的插件并执行
- 收集插件的执行结果

3. 继续对话：
- 将插件执行结果作为新的上下文
- LLM 生成新的响应，可能继续调用插件
- 直到生成完整的答案或达到最大轮次

4. 多轮交互：
- LLM 可能会根据插件返回的结果继续提问或调用其他插件
- 每一轮交互都会被记录在会话历史中
- 整个过程是动态的，直到LLM认为已经生成了完整的答案

---

1. 关于判断是否调用插件：
```python
# 是的，LLM的输出会有两种情况：

# 情况1：普通回复，不需要调用插件
'''
这是一个普通的回答，不需要调用任何插件...
'''

# 情况2：需要调用插件的回复
'''
让我帮你搜索相关信息
<|action_start|><|plugin|>
{
    "name": "arxiv_search",
    "parameters": {
        "query": "...",
        "max_results": 5
    }
}
<|action_end|>
'''
```

2. 关于插件信息的传递，从代码中我们可以看到这是通过系统提示词实现的：

```python
# StreamlitUI.setup_sidebar() 中
meta_prompt = st.sidebar.text_area('系统提示词', value=META_CN)
plugin_prompt = st.sidebar.text_area('插件提示词', value=PLUGIN_CN)

# PLUGIN_CN 就是告诉 LLM 可用插件信息的系统提示词
# META_CN 是基础的系统提示词
```

让我们画个图说明这个提示词系统：

![image](https://github.com/user-attachments/assets/944a5704-0b5b-4898-9b68-d448987a960e)


3. 重要细节：

- 提示词的组成：
```python
# 一个典型的提示词结构：

# 1. 系统基础提示词（META_CN）
'''
你是一个AI助手...
'''

# 2. 插件提示词（PLUGIN_CN）
'''
你可以使用以下插件：
1. arxiv_search: 搜索学术论文
   参数格式：
   {
     "name": "arxiv_search",
     "parameters": {
       "query": "搜索关键词",
       "max_results": "最大结果数"
     }
   }

调用格式：
<|action_start|><|plugin|>
{json格式的参数}
<|action_end|>

示例：...
'''

# 3. 历史对话上下文
[
    {"role": "user", "content": "之前的对话..."},
    {"role": "assistant", "content": "之前的回复..."}
]

# 4. 当前用户输入
{"role": "user", "content": "当前问题"}
```

- 关于每轮对话：
1. 初始化时：
```python
# StreamlitUI.initialize_chatbot() 中设置协议
protocol=Internlm2Protocol(
    tool=dict(
        begin='{start_token}{name}\n',
        start_token='<|action_start|>',
        name_map=dict(
            plugin='<|plugin|>', 
            interpreter='<|interpreter|>'
        ),
        belong='assistant',
        end='<|action_end|>\n',
    ),
)
```

2. 每轮对话中：
- 系统提示词（META_CN）和插件提示词（PLUGIN_CN）会一直保持
- 会带上完整的对话历史
- LLM 可以参考这些信息来决定是否使用插件

4. 工作流程：


![image](https://github.com/user-attachments/assets/d0790a86-1f5b-4e47-a40c-52fd6819cc18)

---

从代码里看，实际的插件提示词模板是在 `PLUGIN_CN` 变量中定义的，但完整的插件信息是在运行时动态填充的。让我们看看这个过程：

1. 首先，在 sidebar 设置时，我们选择了要启用哪些插件
2. 然后，当有插件被选择时，会创建 ActionExecutor
3. 实际的工具说明和格式应该是在 `ActionExecutor` 这个类中生成的。这个类会：
- 收集所有可用插件的信息
- 生成它们的描述和调用格式
- 将这些信息填充到提示词模板中

让我模拟一下这个过程：

```python
# 这个过程大概是这样的（简化版）：

class ArxivSearch:
    name = "arxiv_search"
    description = "搜索学术论文"
    parameters = {
        "query": "搜索关键词",
        "max_results": "最大结果数"
    }

class ActionExecutor:
    def __init__(self, actions):
        self.actions = actions
    
    def get_tools_description(self):
        tools_desc = []
        for action in self.actions:
            desc = f"""
            {action.name}: {action.description}
            参数格式：
            {{
                "name": "{action.name}",
                "parameters": {{
                    # 这里列出参数说明
                }}
            }}
            """
            tools_desc.append(desc)
        
        return "\n".join(tools_desc)

# 当实际使用时
action_executor = ActionExecutor([ArxivSearch()])
tools_description = action_executor.get_tools_description()

# 最终的提示词会变成：
"""
你可以使用如下工具：
arxiv_search: 搜索学术论文
参数格式：
{
    "name": "arxiv_search",
    "parameters": {
        "query": "搜索关键词",
        "max_results": "最大结果数"
    }
}

如果你已经获得足够信息，请直接给出答案. 避免不必要的工具调用! 同时注意你可以使用的工具，不要随意捏造！
"""
```

这里有个重要的设计模式：

关键点是：

1. 插件注册：
- 每个插件（如 ArxivSearch）都需要定义自己的名称、描述和参数格式
- 这些信息会被用来：
  1. 生成给 LLM 的工具说明
  2. 验证 LLM 生成的调用格式是否正确

2. 运行时：
- ActionExecutor 收集所有已启用插件的信息
- 动态生成完整的工具说明
- 这些说明会被填充到提示词模板中

3. 提示词结构：
```
基础提示词（告诉 LLM 它的角色）
+
动态生成的工具说明（告诉它可用的工具）
+
调用格式说明（告诉它如何调用）
+
使用建议（告诉它何时调用）
```

所以，总结一下：
1. LLM 学会调用工具的方式是通过完整的提示词系统
2. 具体的工具信息是由 ActionExecutor 在运行时动态生成的
3. 这种设计允许我们：
   - 动态添加/删除工具
   - 确保 LLM 只调用实际存在的工具
   - 标准化工具的调用格式



---







## 自定义 Agent 智能体

### 体验

MagicMaker 代码逻辑流程图：

![image](https://github.com/user-attachments/assets/b28a6154-569e-4cc0-b398-095932db5e2b)


### 自定义的XXX工具




## 附录：报错

打开 Lagent Web Demo 页面时报错：
```
Traceback (most recent call last):
  File "/root/.conda/envs/lagent/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/exec_code.py", line 88, in exec_func_with_error_handling
    result = func()
  File "/root/.conda/envs/lagent/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py", line 579, in code_to_exec
    exec(code, module.__dict__)
  File "/root/agent_camp4/lagent/examples/internlm2_agent_web_demo.py", line 8, in <module>
    from lagent.actions import ActionExecutor, ArxivSearch, IPythonInterpreter
  File "/root/agent_camp4/lagent/lagent/__init__.py", line 2, in <module>
    from .actions import *  # noqa: F401, F403
  File "/root/agent_camp4/lagent/lagent/actions/__init__.py", line 3, in <module>
    from .action_executor import ActionExecutor
  File "/root/agent_camp4/lagent/lagent/actions/action_executor.py", line 4, in <module>
    from .base_action import BaseAction
  File "/root/agent_camp4/lagent/lagent/actions/base_action.py", line 14, in <module>
    from class_registry import AutoRegister, ClassRegistry
ImportError: cannot import name 'AutoRegister' from 'class_registry' (/root/.conda/envs/lagent/lib/python3.10/site-packages/class_registry/__init__.py)
```

解决：
```
pip install class_registry
```


